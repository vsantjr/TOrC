{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TOrC_TL.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/62840b1eece760d5e42593187847261f/transfer_learning_tutorial.ipynb","timestamp":1604694802955}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Aj5xifWZUpFe"},"source":["## **TOrC: Test Oracle based on CNN**\n","----\n","\n","**Article:** A Method and Experiment to evaluate Deep Neural Networks as\n","Test Oracles for Scientific Software \n","\n","<br>\n","\n","**Author**: Valdivino Alexandre de Santiago JÃºnior\n","\n","<br>\n","\n","**Licence**: GNU GENERAL PUBLIC LICENSE, Version 3 (GPLv3)\n","\n","<br>\n","\n","This notebook is part of the implementation of the **Test Oracle based on CNN** (TOrC) method presented in the abovementioned article. It refers to the architecture configuration **TL** and it is based on a PyTorch tutorial for <a href=\"https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\">transfer learning</a>.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"USKiDAQzQpLn"},"source":["%matplotlib inline\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6tyULOW57g1"},"source":["# Change to \"root\" directory\n","%cd /content/drive/My Drive/Colab Notebooks/pyTorch\n","%ls\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UT5TyIalWS20"},"source":["## **Unzip the Datasets**\n","----\n","\n","Run the commands below only once to unzip all datasets for both dataset profiles (TD and SS). It is assumed that the .zip files have already been uploaded to the Google Drive's directory, and the corresponding directories (TDdataset, SSdataset) have already been created in the Google Drive too.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"1uC_s9K9oOn5"},"source":["#!unzip \"/content/drive/My Drive/Colab Notebooks/pyTorch/TDdataset.zip\" -d \"/content/drive/My Drive/Colab Notebooks/pyTorch/TDdataset\"\n","\n","#!unzip \"/content/drive/My Drive/Colab Notebooks/pyTorch/SSdataset.zip\" -d \"/content/drive/My Drive/Colab Notebooks/pyTorch/SSdataset\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0FcCjo7QpLs"},"source":["from __future__ import print_function, division\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from utilsio import *\n","from utilsdnn import *\n","from phasesdnn import *\n","plt.ion()   # Interactive mode\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7V_8YREq1qoZ"},"source":["# Custom dataset that includes image file paths. Extends \n","# torchvision.datasets.ImageFolder\n","\n","class FileNames(datasets.ImageFolder):\n","    \n","    # Override the __getitem__ method. this is the method that dataloader calls\n","    def __getitem__(self, index):\n","        # This is what ImageFolder normally returns \n","        original_tuple = super(FileNames, self).__getitem__(index)\n","        # The image file path\n","        path = self.imgs[index][0]\n","        # Make a new tuple that includes original and the path\n","        tuple_with_path = (original_tuple + (path,))\n","        return tuple_with_path\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6m4ROGbedPh"},"source":["## **Select the CNN and Dataset Profile**\n","----\n","\n","Select the CNN and dataset profile. Moreover, as mentioned in <a href=\"https://pytorch.org/vision/stable/models.html\">torchvision.models</a>:\n","\n","<br>\n","\n","\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\"\n","\n","<br>\n","\n","Thus, all images are cropped below to 224 x 224 (variable ```hei_wid```), with the exception of Inception v3 whose cropping should be 299 x 299.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"L_8lSuWnQpLv"},"source":["# All relevant outputs are here\n","output_dir = '/content/drive/My Drive/Colab Notebooks/pyTorch/torcout' \n","\n","# Auxiliary variables: all training and validation losses and accuracies; \n","# precision, recall, and F1-score during the training phase\n","all_losses_acc = []\n","d_precision = {}\n","d_recall = {}\n","d_f1 = {}\n","\n","# Epochs and early stopping\n","number_epochs = 100\n","early_proportion = 0.2\n","\n","# Variables related to the CNN selection\n","indic = ''\n","hei_wid = 224\n","bs = 4 # Mini-batch size\n","\n","print(\"Select the CNN: \")\n","print(\"Press '1' for ResNet-18.\")\n","print(\"Press '2' for ResNet-34.\")\n","print(\"Press '3' for ResNeXt-50-32x4d.\")\n","print(\"Press '4' for Wide ResNet-50-2.\")\n","print(\"Press '5' for Inception v3.\")\n","print(\"Press '6' for ResNet-152.\")\n","print(\"Press '7' for DenseNet-161.\\n\\n\")\n","opt = input(\"Enter your choice:\")\n","if opt =='1':\n","   \tprint(\"You selected ResNet-18!\")\n","   \tindic = 'TOrC_TL_ResNet-18'\n","elif opt =='2':\n","   \tprint(\"You selected ResNet-34!\")\n","   \tindic = 'TOrC_TL_ResNet-34' \t\t\n","elif opt =='3':\n","   \tprint(\"You selected ResNeXt-50-32x4d!\")\n","   \tindic = 'TOrC_TL_ResNeXt-50-32x4d'\n","elif opt =='4':\n","   \tprint(\"You selected Wide ResNet-50-2!\")\n","   \tindic = 'TOrC_TL_WideResNet-50-2'\n","elif opt =='5':\n","    hei_wid = 299\n","    print(\"You selected Inception v3!\")\n","    indic = 'TOrC_TL_Inceptionv3'\n","elif opt =='6':\n","   \tprint(\"You selected ResNet-152!\")\n","   \tindic = 'TOrC_TL_ResNet-152'\n","elif opt =='7':\n","   \tprint(\"You selected DenseNet-161!\")\n","   \tindic = 'TOrC_TL_DenseNet-161' \t\t\n","else:\n","   \tprint(\"Invalid Option!\")    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLu0VHbPnfy5"},"source":["# Variable related to the dataset profile selection\n","data_dir = ''\n","print(\"Select the Dataset Profile: \")\n","print(\"Press '1' for TD.\")\n","print(\"Press '2' for SS.\\n\\n\")\n","optdp = input(\"Enter your choice:\")\n","if optdp =='1':\n","  print(\"You selected TD!\")\n","  data_dir = 'TDdataset'\n","  indic += '_TD'\n","elif optdp =='2':\n","  print(\"You selected SS!\")\n","  data_dir = 'SSdataset'\n","  indic += '_SS'\n","else:\n","    print(\"Invalid Option!\")\n","\n","# Best model obtained during the training phase\n","best_model_full = '/content/drive/My Drive/Colab Notebooks/pyTorch/best/best_model_'+indic+'.pt' \n","\n","# Save user options\n","save_user_options(output_dir, best_model_full, indic, early_proportion, number_epochs)\n","\n","# Tranformations for each dataset\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.CenterCrop(hei_wid), \n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.CenterCrop(hei_wid), \n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.CenterCrop(hei_wid), \n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","}\n","\n","image_datasets = {x: FileNames(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val', 'test']}\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=bs,\n","                                             shuffle=True, num_workers=2)\n","              for x in ['train', 'val', 'test']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n","\n","# Just checking the sizes\n","print('\\nDataset sizes: ', dataset_sizes)\n","print('Dataloaders sizes: ', len(dataloaders))\n","class_names = image_datasets['train'].classes\n","class_namesv = image_datasets['val'].classes\n","class_namest = image_datasets['test'].classes\n","print('Training classes: ', class_names)\n","print('Training lengths: ', len(class_names))\n","print('Validation classes: ', class_namesv)\n","print('Validation lengths: ', len(class_namesv))\n","print('Test classes: ', class_namest)\n","print('Test lengths: ', len(class_namest))\n","print('Height x Width: {} x {}'.format(hei_wid,hei_wid))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwlB-OqOP_1e"},"source":["## **Define the Network Configuration**\n","----\n","\n","Since the network configuration is **TL**, we just add one fully-connected layer with two neurons (binary classification).\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"s604GV_vQpL7"},"source":["model_ft = []\n","num_ftrs = -1 # Number of input features for the last layer (fully-connected)\n","num_classes = 2 # Binary classification problem\n","if opt =='1':\n","  model_ft = models.resnet18(pretrained=True)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input features - ResNet-18:', num_ftrs) \n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='2':\n","  model_ft = models.resnet34(pretrained=True)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input features - ResNet-34:', num_ftrs)\n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='3':\n","  model_ft = models.resnext50_32x4d(pretrained=True)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input - features ResNeXt-50-32x4d:', num_ftrs)\n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='4':\n","  model_ft = models.wide_resnet50_2(pretrained=True)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input features - Wide ResNet-50-2:', num_ftrs)\n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='5':\n","  model_ft = models.inception_v3(pretrained=True, aux_logits=False)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input features - Inception v3:', num_ftrs)\n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='6':\n","  model_ft = models.resnet152(pretrained=True)\n","  num_ftrs = model_ft.fc.in_features\n","  print('Input features - ResNet-152:', num_ftrs)\n","  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","elif opt =='7':\n","  model_ft = models.densenet161(pretrained=True)\n","  num_ftrs = model_ft.classifier.in_features\n","  print('Inout features - DeseNet-161:', num_ftrs)\n","  model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","else:\n","  print(\"Invalid Option!\")\n","     \n","model_ft = model_ft.to(device)\n","\n","# Cross-entropy Loss\n","criterion_ft = nn.CrossEntropyLoss()\n","\n","# All parameters are being optimised\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# Decrease learning rate by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","\n","print('\\nNumber of Trainable Parameters!')\n","\n","print('Checking trainable parameters {}: {}'.format(indic, count_parameters(model_ft)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_57Wdpy9y9AA"},"source":["## **Run the Training Phase**\n","----\n","\n","Now, we can run the training phase. Function ```train_model``` also returns values of loss, accuracy, precision, recall, and F1-score obtained during this phase.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"f4PhqdBkQpL-"},"source":["model_ft, all_losses_acc, d_precision, d_recall, d_f1 = train_model(model_ft, \n","                  criterion_ft, optimizer_ft, exp_lr_scheduler, dataloaders,\n","                  dataset_sizes, device)  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7ULwn9_3OBd"},"source":["## **Plots of Metrics of the Training Phase**\n","----\n","\n","Plots showing loss and accuracy values as well as values of precision, recall, and F1-score generated during the training phase.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"ju6gMaj2uVZ9"},"source":["plot_training_metrics(all_losses_acc, d_precision, d_recall, d_f1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2caCiV5P4bZb"},"source":["## **Run the Inference Phase**\n","----\n","\n","Now, we can run the inference phase.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"ubywwcgWef5v"},"source":["test_model(model_ft, criterion_ft, dataloaders, dataset_sizes, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8sRjakw2dDU"},"source":["## **Analyse the Classification**\n","----\n","\n","The function below obtains the misclassified (errors) and correctly classified images for all datasets. The most relevant is due to the test dataset.\n","\n","<br>"]},{"cell_type":"code","metadata":{"id":"s04q7_6dhZHv"},"source":["analyse_classifications(model_ft, dataloaders, device, class_names)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZO93xAVygtA"},"source":["plt.ioff()\n"],"execution_count":null,"outputs":[]}]}